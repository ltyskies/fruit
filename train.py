import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms
import os
import numpy as np
from tqdm import tqdm
import joblib

# --- 0. ç±»åˆ«æ˜ å°„å’Œè‡ªå®šä¹‰ Dataset ---

# 18 ä¸ªæ—§ç±»åˆ« -> 6 ä¸ªæ–°ç±»åˆ« (ç±»åˆ«åç§°éœ€è¦æ ¹æ®å®é™…æ–‡ä»¶å¤¹åç§°ç¡®å®š)
# å‡è®¾æ‚¨çš„åŸå§‹æ–‡ä»¶å¤¹åå°±æ˜¯è¾“å‡ºçš„ç±»åˆ«åç§°ï¼š
OLD_CLASSES = ['Apple', 'Apple_Bad', 'Apple_Good', 'Banana', 'Banana_Bad', 'Banana_Good',
               'Guava', 'Guava_Bad', 'Guava_Good', 'Lemon', 'Lemon_Bad', 'Lemon_Good',
               'Orange', 'Orange_Bad', 'Orange_Good', 'Pomegranate', 'Pomegranate_Bad', 'Pomegranate_Good']

# å®šä¹‰æ–°çš„ç±»åˆ«åç§°
NEW_CLASS_NAMES = ['Apple', 'Banana', 'Guava', 'Lemon', 'Orange', 'Pomegranate']

# å®šä¹‰æ˜ å°„è§„åˆ™
class_mapping = {}
for old_cls in OLD_CLASSES:
    if old_cls.startswith('Apple'):
        class_mapping[old_cls] = 'Apple'
    elif old_cls.startswith('Banana'):
        class_mapping[old_cls] = 'Banana'
    elif old_cls.startswith('Guava'):
        class_mapping[old_cls] = 'Guava'
    # æ³¨æ„ï¼š'Lemon' å’Œ 'Lime' åˆå¹¶ä¸ºä¸€ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬å‘½åä¸º 'Lime/Lemon'
    elif old_cls.startswith('Lemon'):
        class_mapping[old_cls] = 'Lemon'
    elif old_cls.startswith('Orange'):
        class_mapping[old_cls] = 'Orange'
    elif old_cls.startswith('Pomegranate'):
        class_mapping[old_cls] = 'Pomegranate'

# åˆ›å»ºæ–°æ ‡ç­¾åˆ°æ•°å­—ç´¢å¼•çš„æ˜ å°„
new_class_to_idx = {name: i for i, name in enumerate(NEW_CLASS_NAMES)}


# è‡ªå®šä¹‰ Dataset ç±»è¿›è¡Œæ ‡ç­¾é‡æ˜ å°„
class MergedImageFolder(datasets.ImageFolder):
    def __init__(self, root, transform=None):
        super().__init__(root, transform)

        self.new_class_to_idx = new_class_to_idx
        self.new_classes = NEW_CLASS_NAMES

        # åŸå§‹æ ‡ç­¾ (self.targets) åŸºäº ImageFolder çš„é»˜è®¤æ’åºï¼Œéœ€è¦é‡æ–°æ˜ å°„
        self.merged_targets = []
        for old_idx in self.targets:
            # 1. æ‰¾åˆ°åŸå§‹ç±»åˆ«åç§°
            old_class_name = self.classes[old_idx]
            # 2. æ‰¾åˆ°æ–°çš„ç±»åˆ«åç§°
            new_class_name = class_mapping.get(old_class_name)
            if new_class_name is None:
                raise ValueError(f"æ— æ³•æ‰¾åˆ°æ—§ç±»åˆ« {old_class_name} çš„æ–°æ˜ å°„ã€‚è¯·æ£€æŸ¥ class_mappingã€‚")
            # 3. æ‰¾åˆ°æ–°çš„æ•°å­—ç´¢å¼•
            new_idx = self.new_class_to_idx[new_class_name]
            self.merged_targets.append(new_idx)

    def __getitem__(self, index):
        # è°ƒç”¨çˆ¶ç±»çš„ __getitem__ æ¥è·å–å›¾åƒå’ŒåŸå§‹æ ‡ç­¾
        path, _ = self.samples[index]

        # ğŸ› é”™è¯¯ä¿®æ­£ï¼šImageFolder çš„åŠ è½½å™¨æ˜¯ self.loader (æ²¡æœ‰ä¸‹åˆ’çº¿)
        sample = self.loader(path)

        if self.transform is not None:
            sample = self.transform(sample)

        # è¿”å›å›¾åƒå’Œæ–°çš„åˆå¹¶æ ‡ç­¾
        return sample, self.merged_targets[index]

    # è¦†ç›– len ä»¥ä¿æŒä¸€è‡´æ€§
    def __len__(self):
        return len(self.samples)


# --- 1. æ¨¡å‹å®šä¹‰ ---

class SimpleCNN(nn.Module):
    def __init__(self, num_classes):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(32)

        self.fc1 = nn.Linear(8192, 128)
        self.fc2 = nn.Linear(128, num_classes)  # ä½¿ç”¨æ–°çš„ num_classes

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


# --- 2. é…ç½®å’Œæ•°æ®åŠ è½½ ---

ROOT_DIR = 'data/workspace/FruitNet'
TRAIN_DATA_PATH = os.path.join(ROOT_DIR)

# è¶…å‚æ•°
BATCH_SIZE = 32
IMAGE_SIZE = 64
LEARNING_RATE = 0.001
NUM_EPOCHS = 10
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# æ•°æ®é¢„å¤„ç†
data_transforms = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ä½¿ç”¨è‡ªå®šä¹‰çš„ MergedImageFolder åŠ è½½æ•°æ®
DATA_ROOT = os.path.join(ROOT_DIR)
dataset = MergedImageFolder(
    root=DATA_ROOT,
    transform=data_transforms
)

# è‡ªåŠ¨è·å–ç±»åˆ«æ•°é‡ (ç°åœ¨æ˜¯åˆå¹¶åçš„æ•°é‡)
NUM_CLASSES = len(dataset.new_classes)  # ä½¿ç”¨ MergedImageFolder ä¸­çš„æ–°ç±»åˆ«åˆ—è¡¨
CLASS_NAMES = dataset.new_classes
print(f"æ£€æµ‹åˆ°çš„åˆå¹¶ç±»åˆ«æ•°é‡: {NUM_CLASSES}")
print(f"åˆå¹¶åçš„ç±»åˆ«åç§°: {CLASS_NAMES}")

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# åˆ›å»º DataLoader
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

# --- 3. å®ä¾‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---

# å®ä¾‹åŒ–æ¨¡å‹ (ä½¿ç”¨æ–°çš„ NUM_CLASSES=6)
model = SimpleCNN(num_classes=NUM_CLASSES).to(DEVICE)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# --- 4. è¯„ä¼°å‡½æ•° (ä¿æŒä¸å˜) ---

def evaluate_model(model, loader, device, name="Validation", class_names=None):
    model.eval()
    correct = 0
    total = 0
    all_labels = []
    all_preds = []

    print(f"\n--- å¼€å§‹åœ¨ {name} é›†ä¸Šè¯„ä¼° ---")

    with torch.no_grad():
        for inputs, labels in tqdm(loader, desc=f"Evaluating {name}"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    accuracy = 100 * correct / total
    print(f"{name} é›†å‡†ç¡®ç‡: {accuracy:.2f}%")

    print("\n--- é¢„æµ‹ç»“æœç¤ºä¾‹ (å‰ 10 ä¸ª) ---")
    for i in range(min(10, len(all_labels))):
        true_label_idx = all_labels[i]
        pred_label_idx = all_preds[i]

        # ä½¿ç”¨åˆå¹¶åçš„ç±»åˆ«åç§°è¿›è¡Œå±•ç¤º
        true_name = class_names[true_label_idx] if class_names and true_label_idx < len(class_names) else str(
            true_label_idx)
        pred_name = class_names[pred_label_idx] if class_names and pred_label_idx < len(class_names) else str(
            pred_label_idx)

        print(
            f"æ ·æœ¬ {i + 1}: çœŸå®æ ‡ç­¾={true_name} ({true_label_idx}), é¢„æµ‹æ ‡ç­¾={pred_name} ({pred_label_idx}) {'âœ…' if true_label_idx == pred_label_idx else 'âŒ'}")

    return accuracy


# --- 5. è®­ç»ƒå‡½æ•° ---

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, class_names):
    best_val_accuracy = 0.0
    best_model_path = 'FruitNet_best_model_temp.pth'

    print("\n--- å¼€å§‹è®­ç»ƒ ---")

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} [TRAIN]"):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)

        epoch_loss = running_loss / len(train_loader.dataset)
        print(f"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss:.4f}")

        val_accuracy = evaluate_model(model, val_loader, device, name="Validation", class_names=class_names)

        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), best_model_path)
            print(f"*** å½“å‰éªŒè¯é›†å‡†ç¡®ç‡ {best_val_accuracy:.2f}% ä¼˜äºå†å²æœ€é«˜ï¼Œæ¨¡å‹å·²ä¿å­˜åˆ° {best_model_path} ***")

    print("--- è®­ç»ƒå®Œæˆ ---")
    return best_model_path


# --- 6. æ‰§è¡Œè®­ç»ƒå’Œè¯„ä¼° ---

best_model_path = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, DEVICE, CLASS_NAMES)

# åŠ è½½æœ€ä¼˜æ¨¡å‹ç”¨äºæœ€ç»ˆè¯„ä¼°å’Œå¯¼å‡º
best_model = SimpleCNN(num_classes=NUM_CLASSES).to(DEVICE)  # ä½¿ç”¨æ–°çš„ NUM_CLASSES é‡æ–°å®ä¾‹åŒ–
best_model.load_state_dict(torch.load(best_model_path))
best_model.eval()
print("\n--- æœ€ç»ˆè¯„ä¼° (åŠ è½½æœ€ä¼˜æ¨¡å‹) ---")
evaluate_model(best_model, val_loader, DEVICE, name="Final Validation", class_names=CLASS_NAMES)

# --- 7. æ¨¡å‹å¯¼å‡ºå’Œé‡åŒ– ---

print("\n--- å¼€å§‹æ¨¡å‹å¯¼å‡ºå’Œè½»é‡åŒ– ---")

# 7.1. å¯¼å‡ºä¸º .pth (Final Model)
final_pth_path = 'FruitNet_model.pth'
torch.save(best_model.state_dict(), final_pth_path)
print(f"âœ… æ¨¡å‹çŠ¶æ€å­—å…¸å·²ä¿å­˜åˆ° {final_pth_path}")

# 7.2. å¯¼å‡ºä¸º .joblib (ä¿å­˜ state_dict)
try:
    joblib_path = 'FruitNet_model.joblib'
    joblib.dump(best_model.state_dict(), joblib_path)
    print(f"âœ… æ¨¡å‹çŠ¶æ€å­—å…¸å·²ä½¿ç”¨ joblib ä¿å­˜åˆ° {joblib_path}")
except Exception as e:
    print(f"âŒ å¯¼å‡º joblib å¤±è´¥: {e}")

# 7.3. å¯¼å‡ºä¸º .onnx
try:
    onnx_path = 'FruitNet_model.onnx'
    dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)
    torch.onnx.export(
        best_model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=10,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
    )
    print(f"âœ… æ¨¡å‹å·²å¯¼å‡ºä¸º ONNX æ ¼å¼åˆ° {onnx_path}")
except Exception as e:
    print(f"âŒ å¯¼å‡º ONNX å¤±è´¥: {e}")

# 7.4. æ¨¡å‹è½»é‡åŒ– (åŠ¨æ€é‡åŒ–)
quantized_path = 'FruitNet_model_quantized.pth'
try:
    quantized_model = SimpleCNN(num_classes=NUM_CLASSES)
    quantized_model.load_state_dict(torch.load(best_model_path))
    quantized_model.eval()

    quantized_model_cpu = quantized_model.to('cpu')
    quantized_model_dyn = torch.quantization.quantize_dynamic(
        quantized_model_cpu,
        {nn.Conv2d, nn.Linear},
        dtype=torch.qint8
    )

    torch.save(quantized_model_dyn.state_dict(), quantized_path)
    print(f"âœ… æ¨¡å‹å·²é€šè¿‡ **åŠ¨æ€é‡åŒ–** è½»é‡åŒ–å¹¶ä¿å­˜åˆ° {quantized_path}")

    original_size = os.path.getsize(final_pth_path) / (1024 * 1024)
    quantized_size = os.path.getsize(quantized_path) / (1024 * 1024)
    print(f"æ¨¡å‹å¤§å°å¯¹æ¯”: åŸå§‹ ({original_size:.2f} MB) vs. é‡åŒ– ({quantized_size:.2f} MB)")

except Exception as e:
    print(f"âŒ æ¨¡å‹è½»é‡åŒ– (åŠ¨æ€é‡åŒ–) å¤±è´¥: {e}")